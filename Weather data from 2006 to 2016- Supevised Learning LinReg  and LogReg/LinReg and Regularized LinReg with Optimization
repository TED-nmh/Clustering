# Import packages
import numpy as np
import pandas as pd
import sklearn as sk
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

## Import csv to data frame ##
a3 = pd.read_csv('weather_dataset.csv')
a3.shape

# Coding nominal column'precip_type' into binary variable
a3['precip_type']= a3['precip_type'].astype('category')
a3['precip_type'] = a3['precip_type'].cat.codes

# Change 'recording_date_time' type to string type and create new column 'year', which show the year of each row
a3['recording_date_time']= a3['recording_date_time'].astype(str)
a3['year'] = a3['recording_date_time'].str[:+4]

# Split train and test set: consider the data of last 2 years (2015 and 2016) for testing.
a3['year']= a3['year'].astype(int)

#Create train set
a3_train = a3.loc[a3['year'] < 2015]
a3_train = a3_train.drop(['year','recording_date_time'], axis = 1)

#Create test set
a3_test = a3.loc[a3['year'] >= 2015]
a3_test = a3_test.drop(['year','recording_date_time'], axis = 1) 

#Display the shape of training and test sets
print("The shape of train set is")
print(a3_train.shape)
print("The shape of test set is")
print(a3_test.shape)

# Drop 2 columns: 'recording_date_time','year'
a3drop=a3.drop(['recording_date_time','year'],axis = 1)
a3drop.shape

# Calculate correlation coefficient
correlation = a3drop.corr()
correlation

# Draw heatmap for correlation
plt.figure(figsize=(8,8))
sns.heatmap(correlation, annot=True, cmap=plt.cm.Reds)
plt.show()

##Now create a linear model considering the 'temperature' as the target variable and other columns as features 
##Show the test performance (as Mean Absolute Error, MAE) of the model

# Create train set by dropping non-contributing features
a3train = a3_train.drop(['wind_speed', 'wind_bearing', 'visibility', 'pressure','cloud_cover'],axis = 1)

# Create test set by dropping non-contributing features
a3test = a3_test.drop(['wind_speed', 'wind_bearing', 'visibility', 'pressure','cloud_cover',],axis = 1)
a3test.shape

# Create Xtrain,Ytrain from train set and Xtest,Ytest from test set
Xtrain = a3train.iloc[:,[0,2,3]]
Ytrain = a3train.iloc[:,1]

Xtest = a3test.iloc[:,[0,2,3]]
Ytest = a3test.iloc[:,1]

# Linear regression
Ln_model = LinearRegression(normalize=True)

# Fit model using Xtrain, Ytrain
Ln_model.fit(Xtrain, Ytrain)

# Prediction
ypredicted = Ln_model.predict(Xtest)

## Show the test performance
# Calculate MSE
MSE = mean_squared_error(Ytest, ypredicted)
print('The mean squared error is',MSE)

# Calculate MAE
MAE = mean_absolute_error(Ytest, ypredicted)
print('The mean absolute error is',MAE)

##Find the feature which shows maximum correlation with "pressure". 
##Create a linear regression model to predict temperature using these two features ('pressure' and the one which shows maximum correlation).
##Compare the performance of this simplified model with the model developed in the previous question (Q-3). Explain the performance variation, if any.

# Find the feature which shows maximum correlation with "pressure"
print('Based on correlation matrix in Q3, pressure and visibility have the highest correlation coefficient, which is 0.06')

# Create data with pressure, visibility as predictors and temperature as target
a3train1=a3_train.drop(['wind_speed', 'wind_bearing', 'humidity','precip_type','cloud_cover','apparent_temperature',],axis = 1)
a3test1=a3_test.drop(['wind_speed', 'wind_bearing', 'humidity','precip_type','cloud_cover','apparent_temperature',],axis = 1)

Xtrain1 = a3train1.iloc[:,[1,2]]
Ytrain1 = a3train1.iloc[:,0]

Xtest1 = a3test1.iloc[:,[1,2]]
Ytest1 = a3test1.iloc[:,0]

# Create linear regression to predict temperature based on pressure and visibility
# Linear regression
Ln_model1 = LinearRegression(normalize=True)

# Fit model using Xtrain, Ytrain
Ln_model1.fit(Xtrain1, Ytrain1)

# Prediction
ypredicted1 = Ln_model1.predict(Xtest1)

# Calculate MSE
MSE = mean_squared_error(Ytest1, ypredicted1)
print('The mean squared error is',MSE)

# Calculate MAE
MAE = mean_absolute_error(Ytest1, ypredicted1)
print('The mean absolute error is',MAE)

print('Q4 model produces high errors and performs worse than Q3 model')
print('Performance in terms of MSE and MAE of Q4 is higher than that of Q3.')
print('MSE: Q4 = 64.767 > Q3 = 0.945')
print('MAE: Q4 = 6.499 > Q3 = 0.763')
print('Because pressure and visibility have no or low linear relationship with temperature, 0.392781 and-0.005447 particularly')
print('Therefore, both features have low contribution when predicting temperature.')

##Apportion the complete dataset into training and test sets, with an 40-60 split. (6 marks)
#(a) Train a linear regression model without considering overfitting scenario and report the test performance.
#(b) Create an optimal regularised linear regression model and report the test performance.
#(c) Explain the reason behind the performance variation, if any.

q5 = a3.drop(['recording_date_time','year','cloud_cover'], axis = 1)
trainq5 = q5.drop(['temperature'],axis=1)
testq5 = q5.iloc[:,1]
Xtrain2, Xtest2, Ytrain2, Ytest2 = train_test_split(trainq5, testq5, test_size =0.6, random_state=0)

# a. Train a linear regression model without considering overfitting scenario and report the test performance.

# Linear regression
Ln_model2 = LinearRegression(normalize=True)

# Fit model using Xtrain, Ytrain
Ln_model2.fit(Xtrain2, Ytrain2)

# Prediction
ypredicted2 = Ln_model2.predict(Xtest2)

# Calculate MAE
MAE = mean_absolute_error(Ytest2, ypredicted2)
print('The mean absolute error is',MAE)

# b. Create an optimal regularised linear regression model and report the test performance

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso

# Ridge

# Create model
alpha = [.001,.003,.01,.03,1,3,5,10,30]

ridge1 = Ridge(normalize=True)

parameters = {'alpha': [.001,.003,.01,.03,1,3,5,10,30]}

ridge_opt= GridSearchCV(ridge1, parameters, scoring='neg_mean_absolute_error', cv=10)

ridge_opt.fit(Xtrain2, Ytrain2)

# Lasso

# Create model
alpha1 = [1e-10, 1e-5, 1e-3, .001, .01, 1]

lasso1 = Lasso(normalize=True, max_iter=1e5)

parameters1 = {'alpha': [1e-10, 1e-5, 1e-3, .001, .01, 1]}

lasso_opt = GridSearchCV(lasso1, parameters1, scoring='neg_mean_absolute_error', cv = 10)

lasso_opt.fit(Xtrain2, Ytrain2)

print('the best parameter of L2 model is',ridge_opt.best_params_) 
print('the mean absolute error of L2 model is',- ridge_opt.score(Xtest2, Ytest2))
print('the best parameter of L1 model is',lasso_opt.best_params_)
print('the mean absolute error of L1 model is',- lasso_opt.score(Xtest2, Ytest2))
print('the best optimal regularised linear regression model is Ridge-L2, with lambda = 0.003')
